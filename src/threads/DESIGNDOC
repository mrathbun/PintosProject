			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

Group name: OS Team #1 (?)

>> Fill in the names and email addresses of your group members.

Mitchell Rathbun <mrathbun@buffalo.edu>
Ethan Vane <emvane@buffalo.edu>
Wenhua Lin <wenhuali@buffalo.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

   In struct thread:

   -int64_t remainingTicks;
     -This value is used to hold the amount of ticks that the thread is 
      supposed to sleep for. When 0, the thread awakes.

   -struct semaphore sleepSem;
     -Used to block the thread while it is sleeping. When remainingTicks
      gets to 0, sema_up is called and the thread awakes.  


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.
  
   -First a check is completed to make sure that the desired sleep time is 
    greater than 0. If it isn't, then the timer shouldn't sleep. If it is,
    then two members in the thread are modified. The int64_t value 
    remainingTicks is set equal to the value passed in. The thread's member
    semaphore sleepSem is then subjected to a call of sema_down, which 
    effectively blocks the thread. This thread is now reliant on the timer 
    interrupt handler to wake up. For every timer interrupt after the thread
    is blocked, its remainingTicks value is decremented. Once this value 
    reaches 0, sema_up is called on the thread's sleepSem, effectively waking
    the thread.         
   


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

  -While thread_foreach is used to iterate through all of the threads in the 
   system, each thread is checked for a status of blocked and a remainingTicks
   value greater than 0. If either of these comparisons are false, then 
   nothing more is done for that thread. Thus, significant time is only spent 
   on threads that are sleeping, which is unavoidable.
   

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

  -Each thread holds its own semaphore and remainingTicks values. Thus, there
   aren't shared resources between threads that are relevant to the 
   timer_sleep() method. Since there are no shared ressources, there is
   no way for a race condition to occur. 

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

   -Once again, race conditions are avoided by design, rather than explicit 
    synchronization constructs. A timer interrupt will only effect a thread 
    that has been blocked by its sleepSem. Since the thread in timer_sleep()
    is only blocked by sleepSem on the last line of the function, the rest of 
    the function is unaffected by a timer interrupt.
   

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
  
   -Our design choice was made for a couple of reasons. First of all we 
    believe our design does a good job of ensuring synchronization. By 
    ensuring that each thread holds its resources that it relies on for 
    sleeping, there is no overlap between threads. Thus, any possibility 
    of a race condition is handled without the need for a static semaphore. 
    If possible, it is a good idea to eliminate the need for static variables,
    as it introduces a global state that could be problematic when building on
    existing code. Furthermore, our implementation is simple, readable, and 
    efficient. Another possible implementation thought of by our group was to 
    place each sleeping thread in a statically declared list. This would allow 
    us to avoid having to iterate through each thread, and only focus on the 
    sleeping thread. While this solution is more efficient, many other 
    problems arrive. The primary problem is the need for synchronization. 
    Since the dedicated sleep list is a shared resource, any access of this 
    list has to be treated as a critical section. This makes the code more 
    difficult to implement, less clear to read, and less reusable. It also 
    leaves open the possibility of the list being corrupted if synchronization 
    isn't perfectly implemented.   
 

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

   In struct thread: 

   -struct list wait_list;
      -Stores all of the threads waiting on a lock being held by this thread.
       Allows for priority donation to occur. 

   Changed from initial:
   
   -struct list lock_list;
      -Stores all of the locks that a thread currently holds. Used for priority
       donation.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

   -For the data structure, we will be using a list as defined in <list.h>. 
    When a thread attempts to acquire a lock but can't, then it will add itself
    to the list of the thread that is holding the lock. This will give the 
    current thread a chance to reference each one of the priorities in its     
    list when get_priority() is called. Nested donation will then be handled
    through recursion, as get_priority() will be called on every member of the 
    list. This will give each waiting thread the chance to check for a higher
    priority thread in its list. 

   Changed from initial:

   -Each thread has a list of locks that it currently holds.  To determine 
    priority, a thread iterates through all of the threads waiting on the locks
    it is holding. A thread's priority will be the max priority between that 
    thread and all the threads waiting on a lock that it holds.

   -ASCII Representation of nested priority donation:

                     High
                       |
                       v
           Medium <- Lock
              |
              v
     Low <- Lock    

     In this situation, a medium priority process is waiting on a lock held by 
     a low priority process. Furthermore, a high priority process is waiting on
     that same medium priority process. This results in the high priority
     process donating its priority to to the medium priority process, making
     it high priority. That process then further donates its high priority to 
     the low level process, resulting in all processes having a high priority.   

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

   -Releasing a semaphore, lock, or condition variable is handled all in the 
    same way, as both a lock and condition variable use semaphores to 
    implement blocking. As a result, giving access to the highest priority
    threads can be implemented in the same way for each. The initial 
    implementation in pintos woke up the first thread in the waiting queue.
    Thus, it was a first come first served implementation. So, to implement 
    priority scheduling, all that needs to be done is to return the highest
    priority thread in the list rather than the first. This is implemented
    using the <list.h> function list_max, which is passed a pointer to the
    function list_comp_greater.  

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

   -There are two things that can happen when a call to lock_aquire() is made.
    If the lock is not held by another thread, than the lock is acquired. If
    it is, then the thread trying to acquire the lock must be added to the 
    wait_list of the thread that does hold the lock. Every time get_priority()
    is called, it must iterate through its wait_list and call get_priority()
    on all of the threads in the list. This use of recursion allows for each
    thread in the list to pass its donated priority, and not just its base 
    priority. As a result, nested donation is handled through the wait_list
    and recursion inside of get_priority().  

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

   -When lock_release() is called, the lock that held the thread must remove 
    all of the threads waiting on the lock from its wait_list. The highest
    priority thread waiting for the lock will then acquire it. This thread
    will then add all of the threads still waiting on the lock to its 
    wait_list. This is done even though it is the highest priority thread at
    the time, as nested donation could change that ordering.

   Changed from Initial:

   -The thread that releases the lock removes that lock from its lock_list.
    The highest priority waiting thread will add that lock to its lock_list.
    This will be reflected in all future calculations of priority.
       
---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

   -If a thread decrements its priority below the highest priority thread that
    is waiting for cpu, then it should prempt itself. However, it is possible 
    that it decrements its priority, and then is interrupted by the round
    robin scheduler. Since the running thread is no longer the highest priority
    thread, it will yield, and the highest priority thread will be scheduled.
    However, after it returns from the interrupt, the second yield will not 
    have been called. This could be avoided through the use of a lock. Our 
    implementation does all of its work in get_priority(), so this condition
    will not ever be an issue.      

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

   -This design was chosen due to its elegance and flexibility. There were a 
    couple of other options. One would be to update the thread holding the 
    lock's priority using set_priority() whenever a donation was needed. While
    this would be more efficient in most cases, it becomes tricky to properly 
    account for all of the possibilities of nested donation. It is also 
    possible to use a static list to account for all of the threads that 
    are a candidate for donation. However, this again introduces the global 
    state, making it difficult to achieve synchronization. Our design, in 
    contrast, is simple and clean, all while effectively taking into account
    nested donation.     

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

   -In thread.h:

    -In struct thread:
       -int nice;
          -Keeps track of the nice value for the thread. Determines how willing
           thread is to give or take away cpu time.

       -float recent_cpu;
          -Holds the recent_cpu calculation for the thread. Represents how 
           much cpu time the thread has used recently. 

       Changed from Initial:
       
       -made recent_cpu an int represented in fixed-point

   -In thread.c:
  
    -Static variables:
       -static float load_avg;
          -Holds the moving waited average of the number of threads waiting for
           cpu time. This is used in the calculation of recent_cpu. 

       Changed from Initial:
       
       -made load_avg an int represented in fixed-point
 
       -static int ready_threads;
          -Holds the number of ready_threads. This is used in the calculation 
           of load_avg.

       Changed from Initial:

       -No more ready_threads. We simply got the size of ready_list using 
        a helper function.

       -static struct list priority_queues[64];
          -Implemented as an array of size 64 for which each index gives a 
           queue of threads at that given priority.

       Changed from Initial:

       -We did not use priority_queues, instead we just used all_list to hold
        all of the threads.  

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59      A 
 4      4   0   0  62  61  59      A
 8      8   0   0  61  61  59      A          
12     12   0   0  60  61  59      B             
16     12   4   0  60  60  59      A           
20     16   4   0  59  60  59      B             
24     16   8   0  59  59  59      A               
28     20   8   0  58  59  59      B            
32     20  12   0  58  58  59      C            
36     20  12   4  58  58  58      A              

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

   -Yes, there were a couple of ambiguities. First, it was not specified which
    thread should be selected if two or more threads have the same priority.
    We just decided to select the thread at the front of the list, which is in
    this case was the thread whose name was lowest in the alphabet. The other 
    uncertainty came concerning the time quantum used in round robin scheduling.
    Since it is unlikely that the time quantum will be less than 4 ticks, it 
    was not factored into our scheduling.   

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

   -If too much work is done inside the timer interrupt handler, then the ticks
    system will cease to work. However, all of the data needed for the advanced
    scheduler must be updated right after a tick occurs. To combat this 
    problem, we put most of the calculations in the timer interrupt handler,
    but sought to make the code as efficient as possible. Furthermore, we 
    placed calculations for thread members outside of the disabled interrupt 
    context.
  


---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

   -One of the main problems with our design is the amount of work that is done
    in the timer interrupt handler. However, it is difficult to avoid this 
    problem due to the requirements of the problem. If there was more time, a 
    couple of steps could be taken. Since the advanced scheduler is reliant
    on the timer, steps could be taken to reduce the time in the interrupt 
    handler needed for the alarm clock. This could be done by having a static
    list of sleeping threads, instead of simply iterating through all threads.
    Doing this will free up timer in the timer interrupt for all of the threads
    to calculate their priorities.
   
   

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

   -Since C already provided the data type float, we did not find it necessary
    to implement an abstract data type. Instead, we believe the simplest 
    approach is to simply provide stand alone functions for any operation
    involving these float data types. Thus, any calculation involving load_avg
    or recent_cpu can just use these functions in place of the standard C 
    operators. This layer of abstraction is a much cleaner way to implement
    fixed point operations than an implementation with no abstraction. The 
    reason for this is that the code becomes more reusable and transparent to 
    the reader.     

   Changed from Initial:

   -We misunderstood the question and kept referring to it as float in the
    answer.  We actually implemented a 17.14 format fixed-point c file that
    handles fixed-point calculations.  We still used functions to create an
    abstraction layer but did not create an abstract data type.  

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
